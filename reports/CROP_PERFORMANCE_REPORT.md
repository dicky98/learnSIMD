# Crop 算法性能报告

## 1. 测试环境
- **CPU**: 16 核心 (支持 AVX2)
- **GPU**: NVIDIA GeForce RTX 4070
- **系统**: Linux

## 2. 测试场景与结果

我们将 4K 图像分别裁剪为 2K 和 640x640，对比不同实现的平均耗时（单位：ms）。

| 实现方式 | 4K -> 2K (ms) | 4K -> 640x640 (ms) | 特点 |
| :--- | :---: | :---: | :--- |
| **CPU Baseline (memcpy)** | **0.58** | **0.09** | **最快**。利用高度优化的系统库，无额外开销。 |
| **CPU SIMD (AVX2)** | 1.27 | 0.22 | 性能一般。手动指令管理带来了额外负担。 |
| **CPU SIMD + OpenMP** | 4.78 | 0.11 | 较慢。线程创建和同步开销远超计算收益。 |
| **CUDA (Total)** | 5.34 | 4.51 | **瓶颈严重**。PCI-E 传输耗时占比 >98%。 |
| **OpenCV (ROI clone)** | 0.65 | 0.08 | 稳定且高效。与 memcpy 接近。 |

## 3. 详细分析

### CPU 性能分析
- **memcpy 的优越性**: 裁剪操作本质上是不连续内存块的大量拷贝。`std::memcpy` 在底层已经针对 CPU 架构做了极致的 SIMD 对齐优化，手动实现的 AVX2 循环通常难以超越。
- **并行化的负面效应**: 在极短的执行时间（<1ms）内，OpenMP 的线程调度开销会使总体耗时成倍增加。

### GPU 性能分析
- **Kernel 执行**: 裁剪 Kernel 本身运行仅需约 **0.03-0.06ms**，极度高效。
- **传输瓶颈 (The "GPU Tax")**:
    - **H2D (Host to Device)**: ~4.2ms
    - **D2H (Device to Host)**: ~0.2-1.0ms
- **结论**: 除非数据源本身已在显存中（如视频硬解码输出），否则为了裁剪而进行 GPU 传输是极其低效的。

## 4. 结论与建议
- **首选**: 在 CPU 端直接使用 `cv::Mat(roi).clone()` 或手动调用 `memcpy`。
- **禁用**: 不要对简单的裁剪操作使用多线程或单独的 CUDA 调用，除非它是大规模 GPU 流水线的一部分。
